{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclassification with pre-established folders (flow from directory method) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to build a Convolutional Neural Network using the `flow_from_directory` method for three classes:\n",
    "    - Normal\n",
    "    - Not-notmal/Not-pneumonia\n",
    "    - Pneumonia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# System and file management\n",
    "import os\n",
    "import zipfile\n",
    "from glob import glob\n",
    "\n",
    "# Visualization Tools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "from skimage.io import imread\n",
    "\n",
    "# Pandas defaults\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 500\n",
    "\n",
    "# DICOM\n",
    "import pydicom\n",
    "from pydicom.filereader import dcmread\n",
    "\n",
    "# SKLEARN\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# jupyter:\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<style>.container { width:100% !important; }</style>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/' # root\n",
    "CSV_PATH = os.path.join(PATH,'csv') # folder with csv datasets\n",
    "DICOM_PATH = os.path.join(PATH, 'pool') # folder containing all of the dicom files\n",
    "JPG_PATH = os.path.join(PATH, 'pool_jpg') # folder containing all the converted jpg files\n",
    "DESTINATION_PATH = os.path.join(PATH, 'sorted_3') # Folder where the train and test subsets will be located\n",
    "MODELS_PATH = os.path.join(PATH,'model')\n",
    "\n",
    "# Train folder\n",
    "TRAIN_PATH = os.path.join(DESTINATION_PATH, 'train')\n",
    "TRAIN_NORMAL_PATH = os.path.join(TRAIN_PATH, 'normal')\n",
    "TRAIN_NNNP_PATH = os.path.join(TRAIN_PATH, 'nnnp')\n",
    "TRAIN_PNEUMONIA_PATH = os.path.join(TRAIN_PATH, 'pneumonia')\n",
    "\n",
    "# Validation folder\n",
    "VAL_PATH = os.path.join(DESTINATION_PATH, 'validation')\n",
    "VAL_NORMAL_PATH = os.path.join(VAL_PATH, 'normal')\n",
    "VAL_NNNP_PATH = os.path.join(VAL_PATH, 'nnnp')\n",
    "VAL_PNEUMONIA_PATH = os.path.join(VAL_PATH, 'pneumonia')\n",
    "\n",
    "# Test folder\n",
    "TEST_PATH = os.path.join(DESTINATION_PATH, 'test')\n",
    "TEST_NORMAL_PATH = os.path.join(TEST_PATH, 'normal') \n",
    "TEST_NNNP_PATH = os.path.join(TEST_PATH, 'nnnp')\n",
    "TEST_PNEUMONIA_PATH = os.path.join(TEST_PATH, 'pneumonia') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(os.path.join(CSV_PATH, 'cxr_information.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns which contain the output:\n",
    "columns = ['type_0', 'type_1', 'type_2']\n",
    "\n",
    "# Batch size:\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "IMAGE_SIZE = (512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_dataframe(dataframe = train_df,\n",
    "                                                    directory = JPG_PATH,\n",
    "                                                    x_col = \"jpg_file_name\",\n",
    "                                                    y_col = columns,\n",
    "                                                    batch_size = BATCH_SIZE,\n",
    "                                                    seed = SEED,\n",
    "                                                    shuffle = True,\n",
    "                                                    class_mode = 'other',\n",
    "                                                    target_size = IMAGE_SIZE)\n",
    "\n",
    "val_generator = train_datagen.flow_from_dataframe(dataframe = val_df,\n",
    "                                                  directory = JPG_PATH,\n",
    "                                                  x_col = \"jpg_file_name\",\n",
    "                                                  y_col = columns,\n",
    "                                                  batch_size = BATCH_SIZE,\n",
    "                                                  seed = SEED,\n",
    "                                                  shuffle = True,\n",
    "                                                  class_mode = 'other',\n",
    "                                                  target_size = IMAGE_SIZE)\n",
    "\n",
    "test_generator = train_datagen.flow_from_dataframe(dataframe = test_df,\n",
    "                                                   directory = JPG_PATH,\n",
    "                                                   x_col = \"jpg_file_name\",\n",
    "                                                   y_col = columns,\n",
    "                                                   batch_size = BATCH_SIZE,\n",
    "                                                   seed = SEED,\n",
    "                                                   shuffle = True,\n",
    "                                                   class_mode = 'other',\n",
    "                                                   target_size = IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our input feature map is 512x512x3: 150x150 for the image pixels, and 3 for\n",
    "# the three color channels: R, G, and B\n",
    "img_input = layers.Input(shape=(512, 512, 3))\n",
    "\n",
    "# First convolution extracts 16 filters that are 3x3\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "# max-pooling is followed by a dropout to try to avoid overfitting\n",
    "x = layers.Conv2D(16, 3, activation='relu')(img_input)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "\n",
    "# Second convolution extracts 32 filters that are 3x3\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "# max-pooling is followed by a dropout to try to avoid overfitting\n",
    "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "\n",
    "# Third convolution extracts 64 filters that are 3x3\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "# max-pooling is followed by a dropout to try to avoid overfitting\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "x = layers.Dropout(0.25)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten feature map to a 1-dim tensor so we can add fully connected layers\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Create a fully connected layer with ReLU activation and 512 hidden units\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "# Create output layer with a single node and sigmoid activation\n",
    "output = layers.Dense(3, activation='sigmoid')(x)\n",
    "\n",
    "# Create model:\n",
    "# input = input feature map\n",
    "# output = input feature map + stacked convolution/maxpooling layers + fully \n",
    "# connected layer + sigmoid output layer\n",
    "model = Model(img_input, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compilation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tensorflow.compat.v2.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting constants: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VAL = val_generator.n//val_generator.batch_size\n",
    "STEP_SIZE_TEST = test_generator.n//test_generator.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=STEP_SIZE_VAL,\n",
    "                    epochs=1,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"3_class_flow_dir_1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"3_class_flow_dir_1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "pred=model.predict_generator(test_generator,\n",
    "steps=STEP_SIZE_TEST,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bool = (pred >0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pred_bool.astype(int)\n",
    "columns = columns\n",
    "#columns should be the same order of y_col\n",
    "results=pd.DataFrame(predictions, columns=columns)\n",
    "results[\"Filenames\"]=test_generator.filenames\n",
    "ordered_cols=[\"Filenames\"]+columns\n",
    "results=results[ordered_cols]#To get the same column order\n",
    "results.to_csv(\"results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "#Confusion Matrix and Classification Report\n",
    "Y_pred = model.predict_generator(val_generator, STEP_SIZE_VAL)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(val_generator.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = ['Normal', 'NNNP', 'Pneumonia']\n",
    "print(classification_report(val_generator.classes, y_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
