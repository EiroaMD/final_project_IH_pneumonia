{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# System and file management\n",
    "import os\n",
    "import zipfile\n",
    "from glob import glob\n",
    "\n",
    "# Visualization Tools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "from skimage.io import imread\n",
    "\n",
    "# Pandas defaults\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 500\n",
    "\n",
    "# jupyter:\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<style>.container { width:100% !important; }</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DICOM\n",
    "import pydicom\n",
    "from pydicom.filereader import dcmread\n",
    "\n",
    "# Tensoflow\n",
    "import tensorflow\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/' # root\n",
    "CSV_PATH = os.path.join(PATH,'csv') # folder with csv datasets\n",
    "DICOM_PATH = os.path.join(PATH, 'pool') # folder containing all of the dicom files\n",
    "JPG_PATH = os.path.join(PATH, 'pool_jpg') # folder containing all the converted jpg files\n",
    "DESTINATION_PATH = os.path.join(PATH, 'sorted_balanced') # Folder where the train and test subsets will be located\n",
    "MODELS_PATH = os.path.join(PATH,'model')\n",
    "\n",
    "# Train folder\n",
    "TRAIN_PATH = os.path.join(DESTINATION_PATH, 'train')\n",
    "TRAIN_NORMAL_PATH = os.path.join(TRAIN_PATH, 'normal')\n",
    "TRAIN_NNNP_PATH = os.path.join(TRAIN_PATH, 'nnnp')\n",
    "TRAIN_PNEUMONIA_PATH = os.path.join(TRAIN_PATH, 'pneumonia')\n",
    "\n",
    "# Validation folder\n",
    "VAL_PATH = os.path.join(DESTINATION_PATH, 'validation')\n",
    "VAL_NORMAL_PATH = os.path.join(VAL_PATH, 'normal')\n",
    "VAL_NNNP_PATH = os.path.join(VAL_PATH, 'nnnp')\n",
    "VAL_PNEUMONIA_PATH = os.path.join(VAL_PATH, 'pneumonia')\n",
    "\n",
    "# Test folder\n",
    "TEST_PATH = os.path.join(DESTINATION_PATH, 'test')\n",
    "TEST_NORMAL_PATH = os.path.join(TEST_PATH, 'normal') \n",
    "TEST_NNNP_PATH = os.path.join(TEST_PATH, 'nnnp')\n",
    "TEST_PNEUMONIA_PATH = os.path.join(TEST_PATH, 'pneumonia') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8416 images belonging to 2 classes.\n",
      "Found 1804 images belonging to 2 classes.\n",
      "Found 1804 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_PATH,  # This is the source directory for training images\n",
    "        target_size=(512, 512),  # All images will be resized to 150x150\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "        VAL_PATH,  # This is the source directory for training images\n",
    "        target_size=(512, 512),  # All images will be resized to 150x150\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "# Flow validation images in batches of 20 using val_datagen generator\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        TEST_PATH,\n",
    "        target_size=(512, 512),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "\n",
    "\n",
    "json_file = open('2_class_balanced_flow_dir_1.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"2_class_balanced_flow_dir_1.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(lr=0.001),\n",
    "              metrics=['acc', 'AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
